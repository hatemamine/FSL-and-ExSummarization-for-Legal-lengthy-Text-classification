{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/hatemamine/scotuslexglue-preprocesslongtextwithkeybert?scriptVersionId=132282892\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install keybert --quiet\nfrom keybert import KeyBERT\nfrom datasets import load_dataset\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport pandas \nimport numpy as np \nimport re\nimport matplotlib\nimport seaborn as sb \nimport time","metadata":{"execution":{"iopub.status.busy":"2023-06-02T13:43:44.529925Z","iopub.execute_input":"2023-06-02T13:43:44.530206Z","iopub.status.idle":"2023-06-02T13:43:53.990003Z","shell.execute_reply.started":"2023-06-02T13:43:44.530174Z","shell.execute_reply":"2023-06-02T13:43:53.989087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get the number of words \ndef getNumberOfWords(df):\n    maxl=[]\n    for i in range(len(df)):\n        x= df[\"ATexte\"][i].split()\n        if len(x)< 3: print(i)\n        maxl.append(len(x))\n    print(\"Max value element : \", max(maxl))\n    print(\"Min value element : \", min(maxl))\n    return maxl\n    ","metadata":{"execution":{"iopub.status.busy":"2023-06-02T13:32:55.186123Z","iopub.execute_input":"2023-06-02T13:32:55.18642Z","iopub.status.idle":"2023-06-02T13:32:55.192193Z","shell.execute_reply.started":"2023-06-02T13:32:55.186378Z","shell.execute_reply":"2023-06-02T13:32:55.191503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_NumberOfWord(maxl, dataset_name):\n    fig = plt.figure()\n    ax1 = plt.axes()\n    ax1.set_ylabel('words count')\n    ax1.set_xlabel('instances')\n    ind = np.arange(len(maxl))\n    line, =ax1.plot(ind,maxl)\n    plt.axhline(y=512, color=\"red\", linestyle=\"--\")\n    fig.savefig(dataset_name+\"words count.png\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-02T13:32:55.193481Z","iopub.execute_input":"2023-06-02T13:32:55.193877Z","iopub.status.idle":"2023-06-02T13:32:55.211256Z","shell.execute_reply.started":"2023-06-02T13:32:55.19384Z","shell.execute_reply":"2023-06-02T13:32:55.210556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plotNumberSampleforeachCLASS(df, dataset_name):\n    sb.countplot(df.MotsCles)\n    matplotlib.pyplot.ylabel(\"Number of samples per Class\")\n    matplotlib.pyplot.xlabel(\"Class label\")\n    matplotlib.pyplot.savefig(dataset_name+'distribution.png')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-02T13:32:55.213544Z","iopub.execute_input":"2023-06-02T13:32:55.213885Z","iopub.status.idle":"2023-06-02T13:32:55.221434Z","shell.execute_reply.started":"2023-06-02T13:32:55.213846Z","shell.execute_reply":"2023-06-02T13:32:55.220651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def PreprocessLongTextwithKeyBERT(df, n_gram):\n    kw_model = KeyBERT()\n    start_time = time.time()\n    for i in tqdm(range(len(df))):\n        tmpnewtext=\"\"\n        x= df[\"ATexte\"][i].split()\n        #if len(x) < 513 : continue \n        textgen=\"\"\n        for j in range(len(x)):\n            textgen =textgen +\" \"+x[j]\n            if j ==0:continue # for not process the first empty string\n            if (j % 512 ==0) or (len(x)-1==j): \n                keygen = kw_model.extract_keywords(textgen, keyphrase_ngram_range=(n_gram, n_gram), stop_words='english', use_mmr=True, diversity=0.9, top_n=int(max(1, round((512*512)/(len(x)*1), 0))))\n                textgen=\"\"\n                for y in range(len(keygen)):\n                    tmpnewtext=tmpnewtext +\" \"+keygen[y][0]\n                #print(keygen[y][0])\n        df[\"ATexte\"][i]=tmpnewtext\n        #break\n    print(\"--- %s seconds ---\" % (time.time() - start_time))\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-06-02T13:44:29.586974Z","iopub.execute_input":"2023-06-02T13:44:29.587274Z","iopub.status.idle":"2023-06-02T13:44:29.598146Z","shell.execute_reply.started":"2023-06-02T13:44:29.587221Z","shell.execute_reply":"2023-06-02T13:44:29.59741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plotNumberSampleforeachCLASS(df, dataset_name):\n    plot_=sb.countplot(df.MotsCles)\n    #new_ticks = [i.get_text() for i in plot_.get_xticklabels()]\n    #matplotlib.pyplot.xticks(range(0, len(new_ticks), 10), new_ticks[::10], rotation='vertical')\n    matplotlib.pyplot.ylabel(\"Number of samples per Class\")\n    matplotlib.pyplot.xlabel(\"Class label\")\n    matplotlib.pyplot.savefig(dataset_name+'distribution.png')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-02T13:40:47.686231Z","iopub.execute_input":"2023-06-02T13:40:47.686941Z","iopub.status.idle":"2023-06-02T13:40:47.692318Z","shell.execute_reply.started":"2023-06-02T13:40:47.686901Z","shell.execute_reply":"2023-06-02T13:40:47.691452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To ignore warinings\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2023-06-02T13:40:51.848671Z","iopub.execute_input":"2023-06-02T13:40:51.849417Z","iopub.status.idle":"2023-06-02T13:40:51.853602Z","shell.execute_reply.started":"2023-06-02T13:40:51.849379Z","shell.execute_reply":"2023-06-02T13:40:51.852479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = load_dataset(\"lex_glue\", \"scotus\")\nDATASET_NAME='SCOTUS'\nfor x in dataset.keys():\n    #get the train dataset\n    dict = {'ATexte': dataset[x][\"text\"], 'MotsCles': dataset[x][\"label\"]}\n    df = pandas.DataFrame(dict)\n    df[\"ATexte\"]= df[\"ATexte\"].str.lower()\n    for i in range(len(df)):\n        df[\"ATexte\"][i] = re.sub(r'[^a-z ]', '', df[\"ATexte\"][i])\n    print(x,len(df),\"sample\")\n    plotNumberSampleforeachCLASS(df, DATASET_NAME+x)\n    plot_NumberOfWord(getNumberOfWords(df), DATASET_NAME+x)\n    df.to_csv(\"/kaggle/working/\"+DATASET_NAME+x+\".csv\")\n    df = PreprocessLongTextwithKeyBERT(df, 3)\n    df = PreprocessLongTextwithKeyBERT(df, 1)\n    plot_NumberOfWord(getNumberOfWords(df), DATASET_NAME+x+\"KeyBERT\")\n    df.to_csv(\"/kaggle/working/\"+DATASET_NAME+x+\"KeyBERT.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-06-02T13:44:37.169382Z","iopub.execute_input":"2023-06-02T13:44:37.169671Z","iopub.status.idle":"2023-06-02T13:47:02.518046Z","shell.execute_reply.started":"2023-06-02T13:44:37.169638Z","shell.execute_reply":"2023-06-02T13:47:02.516849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas \nimport numpy as np \nimport matplotlib.pyplot as plt\nimport cv2\n#get the train dataset\ndf = pandas.read_csv(\"/kaggle/working/SCOTUStrainKeyBERT.csv\")\n#get the test dataset to augment the training data and live the validation data for evaluation\ndf_test = pandas.read_csv(\"/kaggle/working/SCOTUStestKeyBERT.csv\")\nprint(len(df))\nprint(df.head())","metadata":{"execution":{"iopub.status.busy":"2023-06-02T13:39:51.703072Z","iopub.status.idle":"2023-06-02T13:39:51.703709Z","shell.execute_reply.started":"2023-06-02T13:39:51.703443Z","shell.execute_reply":"2023-06-02T13:39:51.703471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nrandom.seed(1)\ntext1=[]\ntext2=[]\ncorrelated=[]\nlabels=[]\n\nfor i in range (len(df)):\n    row0 =df.iloc[i]\n    # we exclud label 0 and 1 from the generated dataset for training \n    if row0[\"MotsCles\"]==0 or row0[\"MotsCles\"]==1:continue\n    x=0\n    y=0\n    while x < 30:\n        ind = random.randint(0, len(df)-1)\n        if i ==ind : continue\n        # we exclud label 0 and 1 from the generated dataset for training\n        row =df.iloc[ind]\n        if row0[\"MotsCles\"]==row[\"MotsCles\"]:\n            text1.append(row0[\"ATexte\"])\n            text2.append(row[\"ATexte\"])\n            correlated.append(1)\n            labels.append(str(row0[\"MotsCles\"])+\"==\"+str(row[\"MotsCles\"]))\n            x=x+1\n            \n        \n    while y < 30:\n        ind = random.randint(0, len(df)-1)\n        if i ==ind : continue\n        row =df.iloc[ind]\n        if row0[\"MotsCles\"]!=row[\"MotsCles\"]:\n            text1.append(row0[\"ATexte\"])\n            text2.append(row[\"ATexte\"])\n            correlated.append(0)\n            labels.append(str(row0[\"MotsCles\"])+\"!=\"+str(row[\"MotsCles\"]))\n            y=y+1\n    #break\ndict = {'ATexte':text1 , 'MotsCles':text2, 'correlated':correlated, 'labels':labels } \ndf7 = pandas.DataFrame(dict)              \nprint(len(df7))\nprint(df7.head(2))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install sentence-transformers --quiet\ntrain, validate = np.split(df7.sample(frac=1, random_state=42),[int(.9*len(df7))])\n#print(len(df7))\n#print(len(train))\n#print(len(validate))\n#print(train.head())\nfrom sentence_transformers import InputExample\ndev_samples=[]\ntrain_samples=[]\nfor i in range(len(train)):\n        row =train.iloc[i]\n        # two input text and the target (1=similarity, 0=disimilarity)\n        train_samples.append(InputExample(texts=[row[\"ATexte\"], row[\"MotsCles\"]], label=float(row[\"correlated\"])))\n        #break\nfor i in range(len(validate)):\n        row =validate.iloc[i]\n        dev_samples.append(InputExample(texts=[row[\"ATexte\"], row[\"MotsCles\"]], label=float(row[\"correlated\"])))\n        #break\n#print(train_samples[:10])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the generated dataset in a pickle file \nimport pickle\nimport torch\nlexgluescotus = {'train_samples':train_samples, 'dev_samples':dev_samples}\nwith open('/kaggle/working/key_lexgluescotusFSL01.pickle', 'wb') as handle:\n    pickle.dump(lexgluescotus, handle, protocol=pickle.HIGHEST_PROTOCOL)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df =pandas.concat([df, df_test], ignore_index=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nrandom.seed(1)\ntext1=[]\ntext2=[]\ncorrelated=[]\nlabels=[]\n\nfor i in range (len(df)):\n    row0 =df.iloc[i]\n    # we exclud label 0 and 1 from the generated dataset for training \n    if row0[\"MotsCles\"]==0 or row0[\"MotsCles\"]==1:continue\n    x=0\n    y=0\n    while x < 40:\n        ind = random.randint(0, len(df)-1)\n        if i ==ind : continue\n        # we exclud label 0 and 1 from the generated dataset for training\n        row =df.iloc[ind]\n        if row0[\"MotsCles\"]==row[\"MotsCles\"]:\n            text1.append(row0[\"ATexte\"])\n            text2.append(row[\"ATexte\"])\n            correlated.append(1)\n            labels.append(str(row0[\"MotsCles\"])+\"==\"+str(row[\"MotsCles\"]))\n            x=x+1\n            \n        \n    while y < 40:\n        ind = random.randint(0, len(df)-1)\n        if i ==ind : continue\n        row =df.iloc[ind]\n        if row0[\"MotsCles\"]!=row[\"MotsCles\"]:\n            text1.append(row0[\"ATexte\"])\n            text2.append(row[\"ATexte\"])\n            correlated.append(0)\n            labels.append(str(row0[\"MotsCles\"])+\"!=\"+str(row[\"MotsCles\"]))\n            y=y+1\n    #break\ndict = {'ATexte':text1 , 'MotsCles':text2, 'correlated':correlated, 'labels':labels } \ndf7 = pandas.DataFrame(dict)              \nprint(len(df7))\nprint(df7.head(2))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, validate = np.split(df7.sample(frac=1, random_state=42),[int(.9*len(df7))])\n#print(len(df7))\n#print(len(train))\n#print(len(validate))\n#print(train.head())\nfrom sentence_transformers import InputExample\ndev_samples=[]\ntrain_samples=[]\nfor i in range(len(train)):\n        row =train.iloc[i]\n        # two input text and the target (1=similarity, 0=disimilarity)\n        train_samples.append(InputExample(texts=[row[\"ATexte\"], row[\"MotsCles\"]], label=float(row[\"correlated\"])))\n        #break\nfor i in range(len(validate)):\n        row =validate.iloc[i]\n        dev_samples.append(InputExample(texts=[row[\"ATexte\"], row[\"MotsCles\"]], label=float(row[\"correlated\"])))\n        #break\n#print(train_samples[:10])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the generated dataset in a pickle file \nimport pickle\nimport torch\nlexgluescotus = {'train_samples':train_samples, 'dev_samples':dev_samples}\nwith open('/kaggle/working/Augmentkey_lexgluescotusFSL01.pickle', 'wb') as handle:\n    pickle.dump(lexgluescotus, handle, protocol=pickle.HIGHEST_PROTOCOL)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get the train dataset\ndf = pandas.read_csv(\"/kaggle/working/SCOTUStrain.csv\")\nprint(len(df))\nprint(df.head())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nrandom.seed(1)\ntext1=[]\ntext2=[]\ncorrelated=[]\nlabels=[]\n\nfor i in range (len(df)):\n    row0 =df.iloc[i]\n    # we exclud label 0 and 1 from the generated dataset for training \n    if row0[\"MotsCles\"]==0 or row0[\"MotsCles\"]==1:continue\n    x=0\n    y=0\n    while x < 30:\n        ind = random.randint(0, len(df)-1)\n        if i ==ind : continue\n        # we exclud label 0 and 1 from the generated dataset for training\n        row =df.iloc[ind]\n        if row0[\"MotsCles\"]==row[\"MotsCles\"]:\n            text1.append(row0[\"ATexte\"])\n            text2.append(row[\"ATexte\"])\n            correlated.append(1)\n            labels.append(str(row0[\"MotsCles\"])+\"==\"+str(row[\"MotsCles\"]))\n            x=x+1\n            \n        \n    while y < 30:\n        ind = random.randint(0, len(df)-1)\n        if i ==ind : continue\n        row =df.iloc[ind]\n        if row0[\"MotsCles\"]!=row[\"MotsCles\"]:\n            text1.append(row0[\"ATexte\"])\n            text2.append(row[\"ATexte\"])\n            correlated.append(0)\n            labels.append(str(row0[\"MotsCles\"])+\"!=\"+str(row[\"MotsCles\"]))\n            y=y+1\n    #break\ndict = {'ATexte':text1 , 'MotsCles':text2, 'correlated':correlated, 'labels':labels } \ndf7 = pandas.DataFrame(dict)              \nprint(len(df7))\nprint(df7.head(2))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install sentence-transformers --quiet\ntrain, validate = np.split(df7.sample(frac=1, random_state=42),[int(.9*len(df7))])\n#print(len(df7))\n#print(len(train))\n#print(len(validate))\n#print(train.head())\nfrom sentence_transformers import InputExample\ndev_samples=[]\ntrain_samples=[]\nfor i in range(len(train)):\n        row =train.iloc[i]\n        # two input text and the target (1=similarity, 0=disimilarity)\n        train_samples.append(InputExample(texts=[row[\"ATexte\"], row[\"MotsCles\"]], label=float(row[\"correlated\"])))\n        #break\nfor i in range(len(validate)):\n        row =validate.iloc[i]\n        dev_samples.append(InputExample(texts=[row[\"ATexte\"], row[\"MotsCles\"]], label=float(row[\"correlated\"])))\n        #break\n#print(train_samples[:10])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the generated dataset in a pickle file \nimport pickle\nimport torch\nlexgluescotus = {'train_samples':train_samples, 'dev_samples':dev_samples}\nwith open('/kaggle/working/lexgluescotusFSL01.pickle', 'wb') as handle:\n    pickle.dump(lexgluescotus, handle, protocol=pickle.HIGHEST_PROTOCOL)","metadata":{},"execution_count":null,"outputs":[]}]}